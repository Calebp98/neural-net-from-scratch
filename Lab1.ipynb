{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Introduction\n",
    "- Assume a neural network composed of three layers of neurons: input layer, hidden layer and output layer. We wish to train this network so when it is presented with a specific input it will have a certain output.\n",
    "- For this experiment we are going to work with the [MNIST database of handwritten digits](http://yann.lecun.com/exdb/mnist) and train our network to recognise handwritten 0-9 digits. Each image is an  matrix representing a digit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nn.png\" alt=\"neural-net\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The neural network\n",
    "\n",
    "- **Network initialisation:**\n",
    "\n",
    "\n",
    "1. Generate a set of weights between the input and the hidden layer. The input layer should have 784 neurons, one for each pixel of the image.\n",
    "\n",
    "2. Generate a set of weights between the hidden and the output layer. The output layer should have 10 neurons, one for each digit.\n",
    "\n",
    "3. Generate two sets of bias, one for the hidden layer and one for the output layer. Bias is set to 1 and it is an extra input to the neurons in order to avoid zero activation (all inputs equal to 0).   \n",
    "\n",
    "\n",
    "- **Feedforward:**\n",
    "\n",
    "\n",
    "1. Feed an image $\\vec{x_k}$ to the network,\n",
    "   \n",
    "\n",
    "2. Compute the input to each of the neurons of the hidden layer, $act_{h_i} = \\sum_{i=1}^{784}w_i x_i + bias$ and their outputs with the use of the [sigmoid function](http://mathworld.wolfram.com/SigmoidFunction.html),$out_{h_i} = \\frac{1}{(1+e^{-act_i})}$.\n",
    "\n",
    "3. Repeat for the output layer.\n",
    "\n",
    "<img src=\"nn1.png\" alt=\"neural-net\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "- **Error:**\n",
    "\n",
    "1. Calculate the error in the output of the neural network. Given as input a zero image it is expected that the first neuron of the input layer to be fully active, i.e has an output of 1, while the rest neuron to have outputs of 0. Thus the target output would be $target = [1,0,0,\\dots,0]^T$ and given that the output of the neural network is some $output$ the error is given by the formula $E = \\sum_{i=1}^{n}\\frac{1}{2}(target_i-output_i)^2$ where $n$ is the number of neurons in the output layer of the network. \n",
    "\n",
    "<img src=\"nn2.png\" alt=\"neural-net\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "- **Backpropagation:**\n",
    "\n",
    "\n",
    "1. Apply the delta rule between the output and the hidden layers. For the weight $w_{ij}$ we have $\\frac{\\vartheta E}{\\vartheta w_{\\textrm{ij}} }=\\frac{\\vartheta E}{\\vartheta {\\textrm{out}}_{o_i } }\\;\\frac{\\vartheta {\\textrm{out}}_{o_i } }{\\vartheta {\\textrm{act}}_{o_j } }\\;\\frac{\\vartheta {\\textrm{act}}_{o_j } }{\\vartheta w_{\\textrm{ij}} }\\;=-\\left({\\textrm{target}}_i -{\\textrm{output}}_i \\right){\\;\\textrm{out}}_{o_i } \\;\\left(1-{\\textrm{out}}_{o_i } \\right)\\;{\\textrm{out}}_{h_j }$, where $\\delta_{o_i } =\\left({\\textrm{target}}_i -{\\textrm{output}}_i \\right){\\;\\textrm{out}}_{o_i } \\;\\left(1-{\\textrm{out}}_{o_i } \\right)$.\n",
    "\n",
    "2. Update the weights between the hidden and the output layers.  For the weight $w_{ij}$ we have ${\\Delta w}_{\\textrm{ij}} =-\\eta \\delta_{{\\textrm{out}}_i } {\\textrm{out}}_{h_j }$.\n",
    "\n",
    "3. Repeat steps 1 and 2 between the hidden and the input layes and update the weights between them. Here for the weight $w_{ij}$ we have $\\frac{\\vartheta E}{\\vartheta w_{\\textrm{ij}} }=\\frac{\\vartheta E}{\\vartheta {\\textrm{out}}_{h_i } }\\;\\frac{\\vartheta {\\textrm{out}}_{h_i } }{\\vartheta {\\textrm{act}}_{h_j } }\\;\\frac{\\vartheta {\\textrm{act}}_{h_j } }{\\vartheta w_{\\textrm{ij}} }={\\textrm{out}}_{h_i } \\left(1-{\\textrm{out}}_{o_i } \\right)\\left(w_{\\textrm{ij}}^{\\left(2\\right)} \\delta_{o_j }^{\\left(2\\right)} \\right)$, where the terms $w_{\\textrm{ij}}^{\\left(2\\right)} \\delta_j^{\\left(2\\right)}$ are the weights and the delta function of the previous layer.\n",
    "\n",
    "4. Update the weights between the hidden and the input layers. For the weight $w_{ij}$ we have ${\\Delta w}_{\\textrm{ij}} =-\\eta \\delta_{h_i } x_j$.\n",
    "\n",
    "<img src=\"nn3.png\" alt=\"neural-net\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import numpy.matlib \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train set\n",
    "x_train = np.loadtxt('train-images.idx3-ubyte.txt')\n",
    "# Read the train labels\n",
    "trainlabels = np.loadtxt('train-labels.idx1-ubyte.txt')\n",
    "# Read the test set\n",
    "x_test = np.loadtxt('t10k-images.idx3-ubyte.txt')\n",
    "# Read the test labels\n",
    "testlabels = np.loadtxt('t10k-labels.idx1-ubyte.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of image, take the transpose to see it in the right orientation\n",
    "example_image=np.reshape(x_train[:,-1],(28,28))\n",
    "plt.imshow(example_image.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=np.zeros((10,trainlabels.shape[0]))\n",
    "y_test=np.zeros((10,testlabels.shape[0]))\n",
    "\n",
    "for i in range(0,trainlabels.shape[0]):\n",
    "    y_train[trainlabels[i].astype(int),i]=1\n",
    "    \n",
    "for i in range(0,testlabels.shape[0]):\n",
    "    y_test[testlabels[i].astype(int),i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "[p, n] = x_train.shape\n",
    "nlabels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epoch=45\n",
    "n_epoch=6000\n",
    "# n_batch=32\n",
    "n_batch=100\n",
    "n_input_layer=p #number of neurons on input layer\n",
    "# n_hidden_layer=10 #number of neurons on a hidden layer\n",
    "n_hidden_layer=16 #number of neurons on a hidden layer\n",
    "n_output_layer=nlabels #number of neurons on output layer\n",
    "eta=0.1\n",
    "\n",
    "# Add another hidden layer\n",
    "n_hidden_layer2 = 0 ##number of neurons of the hidden layer. 0 deletes this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a simple network\n",
    "#For W1 and W2, columns are the input and the rows are the output.\n",
    "#W1: Number of columns (input) needs to be equal to the number of features \n",
    "#    of the  MNIST digits, thus p. Number of rows (output) should be equal \n",
    "#    to the number of neurons of the hidden layer thus n_hidden_layer.\n",
    "#W2: Number of columns (input) needs to be equal to the number of neurons \n",
    "#    of the hidden layer. Number of rows (output) should be equal to the \n",
    "#    number of digits we wish to find (classification).\n",
    "\n",
    "# create random matrix of weight (uniformally distrubuted)\n",
    "W1=np.random.uniform(0,1,(n_hidden_layer,n_input_layer))\n",
    "# normalise w1 (not really sure why but i can live with this)\n",
    "W1=np.divide(W1,np.matlib.repmat(np.sum(W1,1)[:,None],1,n_input_layer))\n",
    "\n",
    "\n",
    "# W1_5=np.random.uniform(0,1,(n_hidden_layer,n_hidden_layer))\n",
    "# W1_5=np.divide(W1_5,np.matlib.repmat(np.sum(W1_5,1)[:,None],1,n_hidden_layer))\n",
    "\n",
    "\n",
    "# W2=np.random.uniform(0,1,(n_output_layer,n_hidden_layer));\n",
    "# W2=np.divide(W2,np.matlib.repmat(np.sum(W2,1)[:,None],1,n_hidden_layer));\n",
    "\n",
    "\n",
    "W2=np.random.uniform(0,1,(n_output_layer,n_hidden_layer));\n",
    "W2=np.divide(W2,np.matlib.repmat(np.sum(W2,1)[:,None],1,n_hidden_layer));\n",
    "\n",
    "\n",
    "if n_hidden_layer2>0:\n",
    "    \n",
    "#     weights from ouput to second hidden layer\n",
    "    W3=np.random.uniform(0,1,(n_output_layer,n_hidden_layer2));\n",
    "    W3=np.divide(W3,np.matlib.repmat(np.sum(W3,1)[:,None],1,n_hidden_layer2));\n",
    "\n",
    "#     weights from second hidden layer to first hidden layer\n",
    "    W2=np.random.uniform(0,1,(n_hidden_layer2,n_hidden_layer));\n",
    "    W2=np.divide(W2,np.matlib.repmat(np.sum(W2,1)[:,None],1,n_hidden_layer));\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 16), (16, 784))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W2.shape,W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the biases\n",
    "\n",
    "# set all biases to one\n",
    "bias_W1=np.ones((n_hidden_layer,))\n",
    "# bias_W1_5=np.ones((n_hidden_layer))\n",
    "bias_W2=np.ones((n_output_layer,))\n",
    "\n",
    "if n_hidden_layer2>0:\n",
    "    \n",
    "    bias_W3=np.ones((n_output_layer,))\n",
    "    bias_W2=np.ones((n_hidden_layer2,))\n",
    "\n",
    "# Keep track of the network inputs and average error per epoch\n",
    "errors=np.zeros((n_epoch,));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "\n",
    "# do this for every epoch\n",
    "for epoch in range(0,n_epoch):\n",
    "\n",
    "# for every batch\n",
    "    for batch in range(0,n_batch):\n",
    "        \n",
    "        # Input (random element from the dataset)\n",
    "        idx=np.random.randint(0,n); # get some index\n",
    "        x=x_train[:,idx] #set x to a certain image (in vector form)\n",
    "        \n",
    "        # Neural activation: input layer -> hidden layer\n",
    "        act1=np.dot(W1,x)+bias_W1 # calculate activations of next layer (just dot product of image and weights + bias)\n",
    "        # Apply the sigmoid function\n",
    "        out1=1/(1+np.exp(-act1)) #sigmoidal function from maths and stuff\n",
    "    \n",
    "#         # Neural activation: hidden later -> hidden layer\n",
    "#         act1_5=np.dot(W1_5,out1)+bias_W1_5\n",
    "#         out1_5=1/(1+np.exp(-act1_5))\n",
    "    \n",
    "#         # Neural activation: hidden layer -> output layer\n",
    "#         act2=np.dot(W2,out1_5)+bias_W2 # then do the same thing for the next layer\n",
    "#         # Apply the sigmoid function\n",
    "#         out2=1/(1+np.exp(-act2))\n",
    "        \n",
    "\n",
    "        \n",
    "#         # Form the desired output, the correct neuron should have 1 the rest 0\n",
    "#         desired_output=y_train[:,idx] #get label\n",
    "\n",
    "#         # Backpropagation: output layer -> hidden layer\n",
    "#         out2delta=out2*(1-out2)*(out2-desired_output) \n",
    "#         W2=W2-eta*np.outer(out2delta,out1_5)\n",
    "    \n",
    "#         # Backpropagation: hidden layer -> hidden layer\n",
    "#         out1_5delta=out1_5*(1-out1_5)*np.dot(W1_5.T,out2delta)\n",
    "#         W1_5=W1_5-eta*np.outer(out1_5delta,out1) #outer(x,y) is xT * y\n",
    "        \n",
    "#         # Backpropagation: hidden layer -> input layer\n",
    "#         out1delta=out1*(1-out1)*np.dot(W2.T,out2delta)\n",
    "#         W1=W1-eta*np.outer(out1delta,x)\n",
    "        \n",
    "#         # Store the error per epoch\n",
    "#         errors[i]=errors[i]+np.sum((out2-desired_output)*(out2-desired_output))/n_batch\n",
    "        \n",
    "#  \n",
    "\n",
    "# Train the network\n",
    "for epoch in range(0, n_epoch):\n",
    "    for batch in range(0, n_batch):\n",
    "        \n",
    "        # Input (random element from the dataset)\n",
    "        idx = np.random.randint(0,n)\n",
    "        x = x_train[:,idx]\n",
    "        \n",
    "        # Neural activation: input layer -> hidden layer\n",
    "        act1 = np.dot(W1,x) + bias_W1\n",
    "        # Apply the sigmoid function\n",
    "        out1 = 1 / (1 + np.exp(-act1))\n",
    "        \n",
    "        if n_hidden_layer2 > 0:\n",
    "        \n",
    "            # Neural activation: hidden layer -> hidden layer 2\n",
    "            act2 = np.dot(W2, out1) + bias_W2\n",
    "            # Apply the sigmoid function\n",
    "            out2 = 1 / (1 + np.exp(-act2))\n",
    "        \n",
    "            # Neural activation: hidden layer 2 -> output layer\n",
    "            act3=np.dot(W3, out2) + bias_W3\n",
    "            # Apply the sigmoid function\n",
    "            out3 = 1 / (1 + np.exp(-act3))\n",
    "    \n",
    "            # Form the desired output, the correct neuron should have 1 the rest 0\n",
    "            desired_output = y_train[:,idx]\n",
    "\n",
    "            # Backpropagation: output layer -> hidden layer 2\n",
    "            out3delta = out3 * (1 - out3) * (out3 - desired_output)\n",
    "            W3 = W3 - eta * np.outer(out3delta, out2)\n",
    "        \n",
    "            # Backpropagation: hidden layer 2 -> hidden layer\n",
    "            out2delta = out2 * (1 - out2) * np.dot(W3.T, out3delta)\n",
    "            W2 = W2 - eta * np.outer(out2delta,out1)\n",
    "            \n",
    "            # Backpropagation: hidden layer -> input layer\n",
    "            out1delta = out1 * (1 - out1) * np.dot(W2.T, out2delta)\n",
    "            W1 = W1 - eta * np.outer(out1delta, x)\n",
    "            \n",
    "            # Store the error per epoch\n",
    "            errors[epoch] = errors[epoch] + np.sum((out3 - desired_output) * (out3-desired_output)) / n_batch     \n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Neural activation: hidden layer -> output layer \n",
    "            act2 = np.dot(W2, out1) + bias_W2\n",
    "            # Apply the sigmoid function\n",
    "            out2 = 1 / (1 + np.exp(-act2))\n",
    "        \n",
    "            # Form the desired output, the correct neuron should have 1 the rest 0\n",
    "            desired_output = y_train[:,idx]\n",
    "\n",
    "            # Backpropagation: output layer -> hidden layer\n",
    "            out2delta = out2 * (1 - out2) * (out2 - desired_output)\n",
    "            W2 = W2 - eta * np.outer(out2delta, out1)\n",
    "        \n",
    "            # Backpropagation: hidden -> input layer\n",
    "            out1delta = out1 * (1 - out1) * np.dot(W2.T,out2delta)\n",
    "            W1 = W1 - eta * np.outer(out1delta,x)\n",
    "                   \n",
    "            # Store the error per epoch\n",
    "            errors[epoch] = errors[epoch] + np.sum((out2 - desired_output)**2)      \n",
    "\n",
    "        #errors[epoch] = errors[epoch] / n_batch \n",
    "    \n",
    "    errors[epoch] = errors[epoch] / n_batch \n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print ('print epoch number{0}:'.format(i), errors[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.69787852, 0.9098563 , 0.90549731, ..., 0.08585392, 0.07398636,\n",
       "       0.10197016])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the performance\n",
    "plt.plot(errors)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Average error per epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the test set to compute the network's accuracy\n",
    "\n",
    "for j in range(0,n_batch):\n",
    "\n",
    "    # Input (random element from the dataset)\n",
    "    idx=np.random.randint(0,n2); # get some index\n",
    "    x=x_test[:,idx] #set x to a certain image (in vector form)\n",
    "\n",
    "    # Neural activation: input layer -> hidden layer\n",
    "    act1=np.dot(W1,x)+bias_W1 # calculate activations of next layer (just dot product of image and weights + bias)\n",
    "    # Apply the sigmoid function\n",
    "    out1=1/(1+np.exp(-act1)) #sigmoidal function from maths and stuff\n",
    "\n",
    "    # Neural activation: hidden later -> hidden layer\n",
    "    act1_5=np.dot(W1_5,out1)+bias_W1_5\n",
    "    out1_5=1/(1+np.exp(-act1_5))    \n",
    "    \n",
    "    # Neural activation: hidden layer -> output layer\n",
    "    act2=np.dot(W2,out1_5)+bias_W2 # then do the same thing for the next layer\n",
    "    # Apply the sigmoid function\n",
    "    out2=1/(1+np.exp(-act2))\n",
    "    \n",
    "    # Form the desired output, the correct neuron should have 1 the rest 0\n",
    "    desired_output=y_test[:,idx] #get label\n",
    "    \n",
    "# Store the error on the test set\n",
    "errors=np.sum((out2-desired_output)*(out2-desired_output))/n_batch\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02806665369624495"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
